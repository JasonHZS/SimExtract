"""æµ‹è¯•è„šæœ¬ï¼šä½¿ç”¨ SegmentedAttribution æå–ç›¸ä¼¼æ–‡æ¡£ä¸­çš„å…³é”®ç‰‡æ®µ

è¿™ä¸ªè„šæœ¬å±•ç¤ºï¼š
1. ä½¿ç”¨ RandomQuerySampler éšæœºæŠ½å–ä¸€ä¸ªæ–‡æ¡£ä½œä¸ºæŸ¥è¯¢
2. è·å–æœ€ç›¸ä¼¼çš„ Top 3 æ–‡æ¡£
3. ä½¿ç”¨ SegmentedAttribution æå–æ¯ä¸ªç›¸ä¼¼æ–‡æ¡£ä¸­ä¸æŸ¥è¯¢æ–‡æ¡£æœ€ç›¸ä¼¼çš„ç‰‡æ®µï¼ˆTop 3 ç‰‡æ®µï¼‰

æ³¨æ„ï¼š
- éœ€è¦å…ˆå¯åŠ¨ TEI æœåŠ¡ï¼š
  docker run -p 8080:80 --rm -v $PWD/data:/data \
    ghcr.io/huggingface/text-embeddings-inference:cpu-1.2 \
    --model-id sentence-transformers/all-MiniLM-L6-v2
- ChromaDB ä»…ç”¨äºå‘é‡å­˜å‚¨å’Œæ£€ç´¢ï¼Œä¸ç”¨äºå‘é‡åŒ–
- SegmentedAttribution é€šè¿‡ TEI æœåŠ¡è¿›è¡Œå‘é‡åŒ–
"""

import sys
from pathlib import Path
from typing import List

# Add project root to path
project_root = Path(__file__).parent.parent.parent
sys.path.insert(0, str(project_root))

from src.data_pipeline.stores.chroma_store import ChromaStore
from src.data_pipeline.samplers import RandomQuerySampler
from src.attribution.segmented.method import SegmentedAttribution
from src.data_pipeline.vectorizers.tei_vectorizer import TEIVectorizer


def print_separator(char="=", length=100):
    """æ‰“å°åˆ†éš”çº¿"""
    print(char * length)


def print_query_document(doc_id, document, metadata):
    """æ‰“å°æŸ¥è¯¢æ–‡æ¡£ä¿¡æ¯

    Args:
        doc_id: æ–‡æ¡£ID
        document: æ–‡æ¡£æ–‡æœ¬
        metadata: æ–‡æ¡£å…ƒæ•°æ®
    """
    print_separator("=")
    print("ğŸ¯ æŸ¥è¯¢æ–‡æ¡£ï¼ˆéšæœºæŠ½å–ï¼‰")
    print_separator("=")
    print(f"æ–‡æ¡£ID: {doc_id}")
    print(f"æ–‡æ¡£é•¿åº¦: {len(document)} å­—ç¬¦")

    # æ‰“å°å…ƒæ•°æ®
    print(f"\nå…ƒæ•°æ®:")
    for key, value in metadata.items():
        print(f"  - {key}: {value}")

    # æ‰“å°å®Œæ•´æ–‡æ¡£æ–‡æœ¬
    print(f"\næ–‡æ¡£æ–‡æœ¬:")
    print_separator("Â·", 100)
    print(document)
    print_separator("Â·", 100)
    print()


def print_similar_document_with_segments(
    doc_index,
    doc_id,
    document,
    metadata,
    distance,
    attribution_result
):
    """æ‰“å°ç›¸ä¼¼æ–‡æ¡£åŠå…¶æœ€ç›¸ä¼¼ç‰‡æ®µ

    Args:
        doc_index: æ–‡æ¡£ç´¢å¼•
        doc_id: æ–‡æ¡£ID
        document: æ–‡æ¡£æ–‡æœ¬
        metadata: æ–‡æ¡£å…ƒæ•°æ®
        distance: ç›¸ä¼¼åº¦è·ç¦»
        attribution_result: SegmentedAttributionç»“æœ
    """
    print_separator("=")
    print(f"ğŸ“„ ç›¸ä¼¼æ–‡æ¡£ #{doc_index}")
    print_separator("=")
    print(f"æ–‡æ¡£ID: {doc_id}")
    print(f"æ–‡æ¡£é•¿åº¦: {len(document)} å­—ç¬¦")
    print(f"ç›¸ä¼¼åº¦è·ç¦»: {distance:.6f}")

    # è®¡ç®—ç›¸ä¼¼åº¦åˆ†æ•°
    similarity_score = (1 - distance / 2) * 100
    print(f"ç›¸ä¼¼åº¦åˆ†æ•°: {similarity_score:.2f}%")

    # æ‰“å°å…ƒæ•°æ®
    print(f"\nå…ƒæ•°æ®:")
    for key, value in metadata.items():
        print(f"  - {key}: {value}")

    # æ‰“å°å®Œæ•´æ–‡æ¡£æ–‡æœ¬
    print(f"\næ–‡æ¡£æ–‡æœ¬:")
    print_separator("Â·", 100)
    print(document)
    print_separator("Â·", 100)

    # æ‰“å°å½’å› ç‰‡æ®µ
    print(f"\nğŸ” æœ€ç›¸ä¼¼çš„ç‰‡æ®µ (Top 3):")
    print_separator("-", 100)

    top_spans = attribution_result.spans[:3]
    for i, span in enumerate(top_spans, 1):
        print(f"\nç‰‡æ®µ {i}:")
        print(f"  ç›¸ä¼¼åº¦åˆ†æ•°: {span.score:.4f}")
        print(f"  ä½ç½®: [{span.start_idx}:{span.end_idx}]")
        print(f"  é•¿åº¦: {len(span.text)} å­—ç¬¦")
        print(f"  å†…å®¹:")
        print_separator("Â·", 100)
        # æ‰“å°å®Œæ•´ç‰‡æ®µå†…å®¹
        print(f"  {span.text}")
        print_separator("Â·", 100)

    print()


def main():
    """ä¸»å‡½æ•°"""

    # ============== é…ç½®å‚æ•° ==============
    PERSIST_DIR = "./chroma_db"
    COLLECTION_NAME = "xingqiu_chuangye"  # å¯ä¿®æ”¹ä¸ºå…¶ä»–é›†åˆ
    N_RESULTS = 1  # è¿”å›æœ€ç›¸ä¼¼çš„3ä¸ªæ–‡æ¡£
    RANDOM_SEED = None  # è®¾ç½®ä¸ºNoneåˆ™æ¯æ¬¡éšæœºï¼Œè®¾ç½®æ•°å­—åˆ™å¯é‡å¤

    # TEI æœåŠ¡é…ç½®
    TEI_API_URL = "http://localhost:8080/embed"
    TEI_BATCH_SIZE = 64
    TEI_TIMEOUT = 60
    TEI_DIMENSION = 384  # all-MiniLM-L6-v2 çš„ç»´åº¦æ˜¯ 384

    # SegmentedAttribution é…ç½®
    SEGMENTATION_METHOD = "fixed_length"  # "fixed_length" æˆ– "fixed_sentences"
    CHUNK_SIZE = 50  # tokenæ•°é‡ï¼ˆä¸­æ–‡â‰ˆ50å­—ï¼Œè‹±æ–‡â‰ˆ50è¯ï¼‰
    CHUNK_OVERLAP = 10  # é‡å tokenæ•°
    NUM_SENTENCES = 3  # å¥å­åˆ†æ®µæ—¶æ¯æ®µå¥å­æ•°

    print_separator("=")
    print("ğŸ“Š SegmentedAttribution ç›¸ä¼¼ç‰‡æ®µæå–æµ‹è¯•")
    print_separator("=")
    print(f"é…ç½®:")
    print(f"  é›†åˆåç§°: {COLLECTION_NAME}")
    print(f"  ç›¸ä¼¼æ–‡æ¡£æ•°é‡: {N_RESULTS}")
    print(f"  éšæœºç§å­: {RANDOM_SEED if RANDOM_SEED else 'éšæœº'}")
    print(f"  TEI æœåŠ¡: {TEI_API_URL}")
    print(f"  åˆ†æ®µæ–¹æ³•: {SEGMENTATION_METHOD}")
    print(f"  åˆ†å—å¤§å°: {CHUNK_SIZE} tokens")
    print(f"  åˆ†å—é‡å : {CHUNK_OVERLAP} tokens")
    if SEGMENTATION_METHOD == "fixed_sentences":
        print(f"  æ¯æ®µå¥å­æ•°: {NUM_SENTENCES}")
    print_separator("=")
    print()

    try:
        # ============== 1. åˆå§‹åŒ– ChromaStore ==============
        chroma_store = ChromaStore(persist_directory=PERSIST_DIR)

        # éªŒè¯é›†åˆå­˜åœ¨
        collections = chroma_store.list_collections()
        if COLLECTION_NAME not in collections:
            print(f"âœ— é”™è¯¯: é›†åˆ '{COLLECTION_NAME}' ä¸å­˜åœ¨!")
            print(f"å¯ç”¨é›†åˆ: {', '.join(collections)}")
            return

        # è·å–é›†åˆ
        collection = chroma_store.get_collection(COLLECTION_NAME)
        total_docs = collection.count()

        # ============== 2. åˆ›å»ºé‡‡æ ·å™¨ ==============
        sampler = RandomQuerySampler(chroma_store, random_seed=RANDOM_SEED)

        # ============== 3. æ‰§è¡Œé‡‡æ ·å’ŒæŸ¥è¯¢ ==============
        results = sampler.sample_and_query(
            collection_name=COLLECTION_NAME,
            n_results=N_RESULTS
        )

        # ============== 4. æå–æŸ¥è¯¢ç»“æœ ==============
        ids = results['ids'][0]
        documents = results['documents'][0]
        metadatas = results['metadatas'][0]
        distances = results['distances'][0]

        # æŸ¥è¯¢æ–‡æ¡£ï¼ˆç¬¬ä¸€ä¸ªï¼‰
        query_id = ids[0]
        query_doc = documents[0]
        query_metadata = metadatas[0]

        # æ‰“å°æŸ¥è¯¢æ–‡æ¡£
        print_query_document(query_id, query_doc, query_metadata)

        # ============== 5. åˆå§‹åŒ– TEI Vectorizer ==============
        vectorizer = TEIVectorizer(
            api_url=TEI_API_URL,
            batch_size=TEI_BATCH_SIZE,
            max_retries=3,
            timeout=TEI_TIMEOUT,
            dimension=TEI_DIMENSION
        )

        # å¥åº·æ£€æŸ¥
        if not vectorizer.health_check():
            print(f"âœ— é”™è¯¯: TEI æœåŠ¡å¥åº·æ£€æŸ¥å¤±è´¥ï¼")
            print(f"è¯·ç¡®ä¿ TEI æœåŠ¡è¿è¡Œåœ¨ {TEI_API_URL}")
            return

        # ============== 6. åˆå§‹åŒ– SegmentedAttribution ==============
        attribution_config = {
            "segmentation_method": SEGMENTATION_METHOD,
            "chunk_size": CHUNK_SIZE,
            "chunk_overlap": CHUNK_OVERLAP,
            "num_sentences": NUM_SENTENCES,
            "vectorizer": vectorizer  # ä½¿ç”¨ TEI vectorizer
        }

        attribution = SegmentedAttribution(attribution_config)

        # ============== 7. å¯¹æ¯ä¸ªç›¸ä¼¼æ–‡æ¡£æå–ç›¸ä¼¼ç‰‡æ®µ ==============
        print_separator("=")
        print(f"ğŸ”¬ åˆ†æ Top {N_RESULTS} ç›¸ä¼¼æ–‡æ¡£çš„æœ€ç›¸ä¼¼ç‰‡æ®µ")
        print_separator("=")
        print()

        for i in range(1, len(ids)):  # ä»ç¬¬äºŒä¸ªæ–‡æ¡£å¼€å§‹ï¼ˆç¬¬ä¸€ä¸ªæ˜¯æŸ¥è¯¢æ–‡æ¡£ï¼‰
            doc_id = ids[i]
            document = documents[i]
            metadata = metadatas[i]
            distance = distances[i]

            # ä½¿ç”¨ SegmentedAttribution æå–ç›¸ä¼¼ç‰‡æ®µ
            attribution_result = attribution.extract(query_doc, document)

            # æ‰“å°æ–‡æ¡£åŠå…¶æœ€ç›¸ä¼¼ç‰‡æ®µ
            print_similar_document_with_segments(
                doc_index=i,
                doc_id=doc_id,
                document=document,
                metadata=metadata,
                distance=distance,
                attribution_result=attribution_result
            )

        # ============== 8. æ€»ç»“ ==============
        print_separator("=")
        print("âœ… åˆ†æå®Œæˆ")
        print_separator("=")
        print(f"æŸ¥è¯¢æ–‡æ¡£ID: {query_id}")
        print(f"æŸ¥è¯¢æ–‡æ¡£é•¿åº¦: {len(query_doc)} å­—ç¬¦")
        print(f"ç›¸ä¼¼æ–‡æ¡£æ•°é‡: {len(ids) - 1}")
        print(f"è·ç¦»èŒƒå›´: {distances[1]:.6f} ~ {distances[-1]:.6f}")
        print_separator("=")

    except Exception as e:
        print(f"\nâœ— å‘ç”Ÿé”™è¯¯: {e}")
        import traceback
        traceback.print_exc()
        sys.exit(1)


if __name__ == "__main__":
    main()
