"""æµ‹è¯•è„šæœ¬ï¼šä½¿ç”¨ SegmentedAttribution æå–ç›¸ä¼¼æ–‡æ¡£ä¸­çš„å…³é”®ç‰‡æ®µ

è¿™ä¸ªè„šæœ¬å±•ç¤ºï¼š
1. ä½¿ç”¨ RandomQuerySampler éšæœºæŠ½å–ä¸€ä¸ªæ–‡æ¡£ä½œä¸ºæŸ¥è¯¢
2. è·å–æœ€ç›¸ä¼¼çš„ Top 3 æ–‡æ¡£
3. ä½¿ç”¨ SegmentedAttribution æå–æ¯ä¸ªç›¸ä¼¼æ–‡æ¡£ä¸­ä¸æŸ¥è¯¢æ–‡æ¡£æœ€ç›¸ä¼¼çš„ç‰‡æ®µï¼ˆTop 3 ç‰‡æ®µï¼‰

æ³¨æ„ï¼š
- éœ€è¦å…ˆå¯åŠ¨ TEI æœåŠ¡ï¼š
  docker run -p 8080:80 --rm -v $PWD/data:/data \
    ghcr.io/huggingface/text-embeddings-inference:cpu-1.2 \
    --model-id sentence-transformers/all-MiniLM-L6-v2
- ChromaDB ä»…ç”¨äºå‘é‡å­˜å‚¨å’Œæ£€ç´¢ï¼Œä¸ç”¨äºå‘é‡åŒ–
- SegmentedAttribution é€šè¿‡ TEI æœåŠ¡è¿›è¡Œå‘é‡åŒ–
"""

import sys
import argparse
from pathlib import Path
from typing import List

# Add project root to path
project_root = Path(__file__).parent.parent.parent
sys.path.insert(0, str(project_root))

from src.data_pipeline.stores.chroma_store import ChromaStore
from src.data_pipeline.samplers import RandomQuerySampler
from src.attribution.segmented.method import SegmentedAttribution
from src.data_pipeline.vectorizers.tei_vectorizer import TEIVectorizer


def parse_arguments():
    """è§£æå‘½ä»¤è¡Œå‚æ•°"""
    parser = argparse.ArgumentParser(
        description="ä½¿ç”¨ SegmentedAttribution æå–ç›¸ä¼¼æ–‡æ¡£ä¸­çš„å…³é”®ç‰‡æ®µ",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
ç¤ºä¾‹ç”¨æ³•:
  # ä½¿ç”¨é»˜è®¤å‚æ•°
  uv run python src/experiments/test_segmented_attribution.py

  # è‡ªå®šä¹‰åˆ†æ®µå‚æ•°
  uv run python src/experiments/test_segmented_attribution.py --chunk-size 100 --chunk-overlap 20

  # ä½¿ç”¨å¥å­åˆ†æ®µ
  uv run python src/experiments/test_segmented_attribution.py --segmentation-method fixed_sentences --num-sentences 3

  # è‡ªå®šä¹‰é›†åˆå’Œç»“æœæ•°é‡
  uv run python src/experiments/test_segmented_attribution.py --collection my_collection --n 5
        """
    )

    # åŸºæœ¬é…ç½®å‚æ•°
    parser.add_argument(
        "--collection",
        type=str,
        default="xingqiu_chuangye",
        help="ChromaDB é›†åˆåç§° (é»˜è®¤: xingqiu_chuangye)"
    )
    parser.add_argument(
        "--n",
        type=int,
        default=1,
        help="è¿”å›æœ€ç›¸ä¼¼çš„æ–‡æ¡£æ•°é‡ (é»˜è®¤: 1)"
    )
    parser.add_argument(
        "--random-seed",
        type=int,
        default=None,
        help="éšæœºç§å­ï¼Œç”¨äºå¯é‡å¤çš„ç»“æœ (é»˜è®¤: Noneï¼Œæ¯æ¬¡éšæœº)"
    )
    parser.add_argument(
        "--persist-dir",
        type=str,
        default="./chroma_db",
        help="ChromaDB æŒä¹…åŒ–ç›®å½• (é»˜è®¤: ./chroma_db)"
    )

    # SegmentedAttribution é…ç½®
    parser.add_argument(
        "--segmentation-method",
        type=str,
        choices=["fixed_length", "fixed_sentences"],
        default="fixed_length",
        help="åˆ†æ®µæ–¹æ³• (é»˜è®¤: fixed_length)"
    )
    parser.add_argument(
        "--chunk-size",
        type=int,
        default=50,
        help="åˆ†å—å¤§å°ï¼ˆtokenæ•°é‡ï¼Œä¸­æ–‡â‰ˆ50å­—ï¼Œè‹±æ–‡â‰ˆ50è¯ï¼‰ (é»˜è®¤: 50)"
    )
    parser.add_argument(
        "--chunk-overlap",
        type=int,
        default=10,
        help="åˆ†å—é‡å çš„tokenæ•°é‡ (é»˜è®¤: 10)"
    )
    parser.add_argument(
        "--num-sentences",
        type=int,
        default=3,
        help="å¥å­åˆ†æ®µæ—¶æ¯æ®µçš„å¥å­æ•°é‡ï¼ˆä»…åœ¨ segmentation-method=fixed_sentences æ—¶ä½¿ç”¨ï¼‰ (é»˜è®¤: 3)"
    )

    return parser.parse_args()


def print_separator(char="=", length=100):
    """æ‰“å°åˆ†éš”çº¿"""
    print(char * length)


def print_query_document(doc_id, document, metadata):
    """æ‰“å°æŸ¥è¯¢æ–‡æ¡£ä¿¡æ¯

    Args:
        doc_id: æ–‡æ¡£ID
        document: æ–‡æ¡£æ–‡æœ¬
        metadata: æ–‡æ¡£å…ƒæ•°æ®
    """
    print_separator("=")
    print("ğŸ¯ æŸ¥è¯¢æ–‡æ¡£ï¼ˆéšæœºæŠ½å–ï¼‰")
    print_separator("=")
    print(f"æ–‡æ¡£ID: {doc_id}")
    print(f"æ–‡æ¡£é•¿åº¦: {len(document)} å­—ç¬¦")

    # æ‰“å°å…ƒæ•°æ®
    print(f"\nå…ƒæ•°æ®:")
    for key, value in metadata.items():
        print(f"  - {key}: {value}")

    # æ‰“å°å®Œæ•´æ–‡æ¡£æ–‡æœ¬
    print(f"\næ–‡æ¡£æ–‡æœ¬:")
    print_separator("Â·", 100)
    print(document)
    print_separator("Â·", 100)
    print()


def print_similar_document_with_segments(
    doc_index,
    doc_id,
    document,
    metadata,
    distance,
    attribution_result
):
    """æ‰“å°ç›¸ä¼¼æ–‡æ¡£åŠå…¶æœ€ç›¸ä¼¼ç‰‡æ®µ

    Args:
        doc_index: æ–‡æ¡£ç´¢å¼•
        doc_id: æ–‡æ¡£ID
        document: æ–‡æ¡£æ–‡æœ¬
        metadata: æ–‡æ¡£å…ƒæ•°æ®
        distance: ç›¸ä¼¼åº¦è·ç¦»
        attribution_result: SegmentedAttributionç»“æœ
    """
    print_separator("=")
    print(f"ğŸ“„ ç›¸ä¼¼æ–‡æ¡£ #{doc_index}")
    print_separator("=")
    print(f"æ–‡æ¡£ID: {doc_id}")
    print(f"æ–‡æ¡£é•¿åº¦: {len(document)} å­—ç¬¦")
    print(f"ç›¸ä¼¼åº¦è·ç¦»: {distance:.6f}")

    # è®¡ç®—ç›¸ä¼¼åº¦åˆ†æ•°
    similarity_score = (1 - distance / 2) * 100
    print(f"ç›¸ä¼¼åº¦åˆ†æ•°: {similarity_score:.2f}%")

    # æ‰“å°å…ƒæ•°æ®
    print(f"\nå…ƒæ•°æ®:")
    for key, value in metadata.items():
        print(f"  - {key}: {value}")

    # æ‰“å°å®Œæ•´æ–‡æ¡£æ–‡æœ¬
    print(f"\næ–‡æ¡£æ–‡æœ¬:")
    print_separator("Â·", 100)
    print(document)
    print_separator("Â·", 100)

    # æ‰“å°å½’å› ç‰‡æ®µ
    print(f"\nğŸ” æœ€ç›¸ä¼¼çš„ç‰‡æ®µ (Top 3):")
    print_separator("-", 100)

    top_spans = attribution_result.spans[:3]
    for i, span in enumerate(top_spans, 1):
        print(f"\nç‰‡æ®µ {i}:")
        print(f"  ç›¸ä¼¼åº¦åˆ†æ•°: {span.score:.4f}")
        print(f"  ä½ç½®: [{span.start_idx}:{span.end_idx}]")
        print(f"  é•¿åº¦: {len(span.text)} å­—ç¬¦")
        print(f"  å†…å®¹:")
        print_separator("Â·", 100)
        # æ‰“å°å®Œæ•´ç‰‡æ®µå†…å®¹
        print(f"  {span.text}")
        print_separator("Â·", 100)

    print()


def main():
    """ä¸»å‡½æ•°"""

    # è§£æå‘½ä»¤è¡Œå‚æ•°
    args = parse_arguments()

    # TEI æœåŠ¡é…ç½®ï¼ˆç¡¬ç¼–ç ï¼‰
    TEI_API_URL = "http://localhost:8080/embed"
    TEI_BATCH_SIZE = 64
    TEI_TIMEOUT = 60
    TEI_DIMENSION = 384  # all-MiniLM-L6-v2 çš„ç»´åº¦æ˜¯ 384

    print_separator("=")
    print("ğŸ“Š SegmentedAttribution ç›¸ä¼¼ç‰‡æ®µæå–æµ‹è¯•")
    print_separator("=")
    print(f"é…ç½®:")
    print(f"  é›†åˆåç§°: {args.collection}")
    print(f"  ç›¸ä¼¼æ–‡æ¡£æ•°é‡: {args.n}")
    print(f"  éšæœºç§å­: {args.random_seed if args.random_seed else 'éšæœº'}")
    print(f"  TEI æœåŠ¡: {TEI_API_URL}")
    print(f"  åˆ†æ®µæ–¹æ³•: {args.segmentation_method}")
    print(f"  åˆ†å—å¤§å°: {args.chunk_size} tokens")
    print(f"  åˆ†å—é‡å : {args.chunk_overlap} tokens")
    if args.segmentation_method == "fixed_sentences":
        print(f"  æ¯æ®µå¥å­æ•°: {args.num_sentences}")
    print_separator("=")
    print()

    try:
        # ============== 1. åˆå§‹åŒ– ChromaStore ==============
        chroma_store = ChromaStore(persist_directory=args.persist_dir)

        # éªŒè¯é›†åˆå­˜åœ¨
        collections = chroma_store.list_collections()
        if args.collection not in collections:
            print(f"âœ— é”™è¯¯: é›†åˆ '{args.collection}' ä¸å­˜åœ¨!")
            print(f"å¯ç”¨é›†åˆ: {', '.join(collections)}")
            return

        # è·å–é›†åˆ
        collection = chroma_store.get_collection(args.collection)
        total_docs = collection.count()

        # ============== 2. åˆ›å»ºé‡‡æ ·å™¨ ==============
        sampler = RandomQuerySampler(chroma_store, random_seed=args.random_seed)

        # ============== 3. æ‰§è¡Œé‡‡æ ·å’ŒæŸ¥è¯¢ ==============
        results = sampler.sample_and_query(
            collection_name=args.collection,
            n_results=args.n
        )

        # ============== 4. æå–æŸ¥è¯¢ç»“æœ ==============
        ids = results['ids'][0]
        documents = results['documents'][0]
        metadatas = results['metadatas'][0]
        distances = results['distances'][0]

        # æŸ¥è¯¢æ–‡æ¡£ï¼ˆç¬¬ä¸€ä¸ªï¼‰
        query_id = ids[0]
        query_doc = documents[0]
        query_metadata = metadatas[0]

        # æ‰“å°æŸ¥è¯¢æ–‡æ¡£
        print_query_document(query_id, query_doc, query_metadata)

        # ============== 5. åˆå§‹åŒ– TEI Vectorizer ==============
        vectorizer = TEIVectorizer(
            api_url=TEI_API_URL,
            batch_size=TEI_BATCH_SIZE,
            max_retries=3,
            timeout=TEI_TIMEOUT,
            dimension=TEI_DIMENSION
        )

        # å¥åº·æ£€æŸ¥
        if not vectorizer.health_check():
            print(f"âœ— é”™è¯¯: TEI æœåŠ¡å¥åº·æ£€æŸ¥å¤±è´¥ï¼")
            print(f"è¯·ç¡®ä¿ TEI æœåŠ¡è¿è¡Œåœ¨ {TEI_API_URL}")
            return

        # ============== 6. åˆå§‹åŒ– SegmentedAttribution ==============
        attribution_config = {
            "segmentation_method": args.segmentation_method,
            "chunk_size": args.chunk_size,
            "chunk_overlap": args.chunk_overlap,
            "num_sentences": args.num_sentences,
            "vectorizer": vectorizer  # ä½¿ç”¨ TEI vectorizer
        }

        attribution = SegmentedAttribution(attribution_config)

        # ============== 7. å¯¹æ¯ä¸ªç›¸ä¼¼æ–‡æ¡£æå–ç›¸ä¼¼ç‰‡æ®µ ==============
        print_separator("=")
        print(f"ğŸ”¬ åˆ†æ Top {args.n} ç›¸ä¼¼æ–‡æ¡£çš„æœ€ç›¸ä¼¼ç‰‡æ®µ")
        print_separator("=")
        print()

        for i in range(1, len(ids)):  # ä»ç¬¬äºŒä¸ªæ–‡æ¡£å¼€å§‹ï¼ˆç¬¬ä¸€ä¸ªæ˜¯æŸ¥è¯¢æ–‡æ¡£ï¼‰
            doc_id = ids[i]
            document = documents[i]
            metadata = metadatas[i]
            distance = distances[i]

            # ä½¿ç”¨ SegmentedAttribution æå–ç›¸ä¼¼ç‰‡æ®µ
            attribution_result = attribution.extract(query_doc, document)

            # æ‰“å°æ–‡æ¡£åŠå…¶æœ€ç›¸ä¼¼ç‰‡æ®µ
            print_similar_document_with_segments(
                doc_index=i,
                doc_id=doc_id,
                document=document,
                metadata=metadata,
                distance=distance,
                attribution_result=attribution_result
            )

        # ============== 8. æ€»ç»“ ==============
        print_separator("=")
        print("âœ… åˆ†æå®Œæˆ")
        print_separator("=")
        print(f"æŸ¥è¯¢æ–‡æ¡£ID: {query_id}")
        print(f"æŸ¥è¯¢æ–‡æ¡£é•¿åº¦: {len(query_doc)} å­—ç¬¦")
        print(f"ç›¸ä¼¼æ–‡æ¡£æ•°é‡: {len(ids) - 1}")
        print(f"è·ç¦»èŒƒå›´: {distances[1]:.6f} ~ {distances[-1]:.6f}")
        print_separator("=")

    except Exception as e:
        print(f"\nâœ— å‘ç”Ÿé”™è¯¯: {e}")
        import traceback
        traceback.print_exc()
        sys.exit(1)


if __name__ == "__main__":
    main()
