"""æµ‹è¯•è„šæœ¬ï¼šä½¿ç”¨ SparseAttribution åœ¨ Chroma ç›¸ä¼¼æ£€ç´¢ç»“æœä¸Šåšå½’å› 

è¿™ä¸ªè„šæœ¬å¯¹é½ `src/experiments/test_segmented_attribution.py` çš„æµ‹è¯•æ–¹å¼ï¼š
1) ä» Chroma collection éšæœºæŠ½å–ä¸€ä¸ªæ–‡æ¡£ä½œä¸ºæŸ¥è¯¢ï¼ˆQueryï¼‰
2) ä½¿ç”¨åŒä¸€ collection çš„å‘é‡æ£€ç´¢æ‹¿åˆ° Top-N ç›¸ä¼¼æ–‡æ¡£ï¼ˆé€šå¸¸åŒ…å« Query è‡ªèº«ï¼‰
3) å¯¹æ¯ä¸ªç›¸ä¼¼æ–‡æ¡£ï¼ˆé™¤ Queryï¼‰æ‰§è¡Œ SparseAttributionï¼Œè¾“å‡ºï¼š
   - token-level çš„ top contributing tokens
   - sliding window çš„ top spans

æ³¨æ„ï¼š
- éœ€è¦å®‰è£… FlagEmbedding: pip install FlagEmbedding
- é¦–æ¬¡è¿è¡Œä¼šè‡ªåŠ¨ä¸‹è½½/åŠ è½½ BGE-M3 æ¨¡å‹ï¼ˆä½“ç§¯è¾ƒå¤§ï¼‰
- é»˜è®¤ä» `config/attribution.yaml` è¯»å– sparse é…ç½®ä½œä¸ºé»˜è®¤å€¼ï¼ˆCLI ä¼šè¦†ç›–ï¼‰
"""

import sys
import argparse
import yaml
from pathlib import Path

# Add project root to path
project_root = Path(__file__).parent.parent.parent
sys.path.insert(0, str(project_root))

from src.data_pipeline.stores.chroma_store import ChromaStore
from src.data_pipeline.samplers import RandomQuerySampler
from src.attribution.token_wise import SparseAttribution


def load_config():
    """åŠ è½½é…ç½®æ–‡ä»¶"""
    config_path = project_root / "config" / "attribution.yaml"
    if config_path.exists():
        with open(config_path, 'r', encoding='utf-8') as f:
            config = yaml.safe_load(f)
            return config.get('sparse', {})
    return {}


def parse_arguments(config_defaults=None):
    """è§£æå‘½ä»¤è¡Œå‚æ•°
    
    Args:
        config_defaults: ä»é…ç½®æ–‡ä»¶è¯»å–çš„é»˜è®¤å€¼å­—å…¸
    """
    if config_defaults is None:
        config_defaults = {}
    
    parser = argparse.ArgumentParser(
        description="ä½¿ç”¨ SparseAttribution åœ¨ Chroma ç›¸ä¼¼æ£€ç´¢ç»“æœä¸Šåšå½’å› ",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
ç¤ºä¾‹ç”¨æ³•:
  # ä½¿ç”¨é»˜è®¤å‚æ•°è¿è¡Œï¼ˆä» Chroma collection æŠ½å–æ•°æ®ï¼‰
  uv run python src/experiments/test_sparse_attribution.py

  # æŒ‡å®š collection / è¿”å›ç›¸ä¼¼æ–‡æ¡£æ•°é‡
  uv run python src/experiments/test_sparse_attribution.py --collection xingqiu_chuangye --n 5

  # è°ƒæ•´ sparse sliding window å‚æ•°
  uv run python src/experiments/test_sparse_attribution.py --window-size 50 --window-overlap 40

  # æ˜¾ç¤ºæ›´å¤š top tokensï¼ˆæ‰“å°ç”¨ï¼‰
  uv run python src/experiments/test_sparse_attribution.py --top-n 20
        """
    )

    # åŸºæœ¬é…ç½®å‚æ•°ï¼ˆå¯¹é½ segmented testï¼‰
    parser.add_argument(
        "--collection",
        type=str,
        default="xingqiu_chuangye",
        help="ChromaDB é›†åˆåç§° (é»˜è®¤: xingqiu_chuangye)"
    )
    parser.add_argument(
        "--n",
        type=int,
        default=1,
        help="è¿”å›æœ€ç›¸ä¼¼çš„æ–‡æ¡£æ•°é‡ (é»˜è®¤: 1ï¼Œé€šå¸¸ä¼šåŒ…å« query è‡ªèº«)"
    )
    parser.add_argument(
        "--random-seed",
        type=int,
        default=None,
        help="éšæœºç§å­ï¼Œç”¨äºå¯é‡å¤çš„ç»“æœ (é»˜è®¤: Noneï¼Œæ¯æ¬¡éšæœº)"
    )
    parser.add_argument(
        "--persist-dir",
        type=str,
        default="./chroma_db",
        help="ChromaDB æŒä¹…åŒ–ç›®å½• (é»˜è®¤: ./chroma_db)"
    )

    # SparseAttribution é…ç½®ï¼ˆä½¿ç”¨é…ç½®æ–‡ä»¶çš„å€¼ä½œä¸ºé»˜è®¤å€¼ï¼‰
    parser.add_argument(
        "--model-name",
        type=str,
        default=config_defaults.get("model_name", "BAAI/bge-m3"),
        help=f"BGE-M3 model name or path (é»˜è®¤: {config_defaults.get('model_name', 'BAAI/bge-m3')})"
    )
    parser.add_argument(
        "--window-size",
        type=int,
        default=config_defaults.get("window_size", 50),
        help=f"æ»‘åŠ¨çª—å£ token æ•°é‡ (é»˜è®¤: {config_defaults.get('window_size', 50)})"
    )
    parser.add_argument(
        "--window-overlap",
        type=int,
        default=config_defaults.get("window_overlap", 40),
        help=f"æ»‘åŠ¨çª—å£é‡å  token æ•°é‡ (é»˜è®¤: {config_defaults.get('window_overlap', 40)})"
    )
    parser.add_argument(
        "--top-n",
        type=int,
        default=10,
        help="æ‰“å° Top-N è´¡çŒ®æœ€é«˜çš„ token (é»˜è®¤: 10)"
    )
    parser.add_argument(
        "--top-k-spans",
        type=int,
        default=config_defaults.get("top_k_spans", 5),
        help=f"è¿”å› Top-K è´¡çŒ®æœ€é«˜çš„ç‰‡æ®µ (é»˜è®¤: {config_defaults.get('top_k_spans', 5)})"
    )
    parser.add_argument(
        "--no-fp16",
        action="store_true",
        help=f"ç¦ç”¨åŠç²¾åº¦è®¡ç®—ï¼ˆé»˜è®¤{'å¯ç”¨' if config_defaults.get('use_fp16', True) else 'ç¦ç”¨'} fp16ï¼‰"
    )

    return parser.parse_args()


def print_separator(char="=", length=100):
    """æ‰“å°åˆ†éš”çº¿"""
    print(char * length)


def print_query_document(doc_id, document, metadata):
    """æ‰“å°æŸ¥è¯¢æ–‡æ¡£ä¿¡æ¯"""
    print_separator("=")
    print("ğŸ¯ æŸ¥è¯¢æ–‡æ¡£ï¼ˆéšæœºæŠ½å–ï¼‰")
    print_separator("=")
    print(f"æ–‡æ¡£ID: {doc_id}")
    print(f"æ–‡æ¡£é•¿åº¦: {len(document)} å­—ç¬¦")

    print("\nå…ƒæ•°æ®:")
    for key, value in metadata.items():
        print(f"  - {key}: {value}")

    print("\næ–‡æ¡£æ–‡æœ¬:")
    print_separator("Â·", 100)
    print(document)
    print_separator("Â·", 100)
    print()


def print_similar_document_with_sparse_attribution(
    doc_index,
    doc_id,
    document,
    metadata,
    distance,
    attribution_result,
    top_tokens,
):
    """æ‰“å°ç›¸ä¼¼æ–‡æ¡£åŠå…¶ sparse attribution ç»“æœ"""
    print_separator("=")
    print(f"ğŸ“„ ç›¸ä¼¼æ–‡æ¡£ #{doc_index}")
    print_separator("=")
    print(f"æ–‡æ¡£ID: {doc_id}")
    print(f"æ–‡æ¡£é•¿åº¦: {len(document)} å­—ç¬¦")
    print(f"ç›¸ä¼¼åº¦è·ç¦»: {distance:.6f}")

    similarity_score = (1 - distance / 2) * 100
    print(f"ç›¸ä¼¼åº¦åˆ†æ•°: {similarity_score:.2f}%")

    print("\nå…ƒæ•°æ®:")
    for key, value in metadata.items():
        print(f"  - {key}: {value}")

    print("\næ–‡æ¡£æ–‡æœ¬:")
    print_separator("Â·", 100)
    print(document)
    print_separator("Â·", 100)

    print("\nğŸ† Top Contributing Tokens:")
    print_separator("-", 100)
    if top_tokens:
        print(f"\n{'æ’å':<6}{'Token':<20}{'Score':<15}{'Normalized':<15}")
        print_separator("-", 60)
        for i, token_info in enumerate(top_tokens, 1):
            print(
                f"{i:<6}"
                f"{token_info['token']:<20}"
                f"{token_info['score']:<15.6f}"
                f"{token_info['normalized_score']:<15.4f}"
            )
    else:
        print("âš ï¸ æ²¡æœ‰æ‰¾åˆ°å…±åŒçš„ token")

    print("\nğŸ“‘ Top Attribution Spans (Sliding Window):")
    print_separator("-", 100)
    print("å½’å› å…ƒä¿¡æ¯:")
    print(f"  - total_lexical_score: {attribution_result.metadata.get('total_lexical_score', 0.0):.6f}")
    print(f"  - num_contributing_tokens: {attribution_result.metadata.get('num_contributing_tokens', 0)}")
    print(f"  - total_windows_analyzed: {attribution_result.metadata.get('total_windows_analyzed', 0)}")
    print(f"  - window_size: {attribution_result.metadata.get('window_size', None)}")
    print(f"  - window_overlap: {attribution_result.metadata.get('window_overlap', None)}")

    if not attribution_result.spans:
        print("\nâš ï¸ æœªæå–åˆ° spansï¼ˆå¯èƒ½æ²¡æœ‰å…±åŒ token æˆ–æ–‡æœ¬è¿‡çŸ­ï¼‰")
        print()
        return

    for i, span in enumerate(attribution_result.spans, 1):
        print(f"\nã€ç‰‡æ®µ {i}ã€‘")
        print(f"  ä½ç½®: [{span.start_idx}:{span.end_idx}]")
        print(f"  å½’ä¸€åŒ–åˆ†æ•°: {span.score:.4f}")
        if span.metadata and "raw_score" in span.metadata:
            print(f"  åŸå§‹åˆ†æ•°: {span.metadata['raw_score']:.6f}")
        if span.metadata and "token_count" in span.metadata:
            print(f"  çª—å£ Token æ•°: {span.metadata['token_count']}")
        if span.metadata and "contributing_tokens" in span.metadata:
            print(f"  è´¡çŒ® Token æ•°: {span.metadata['contributing_tokens']}")
        print("  å†…å®¹:")
        print_separator("Â·", 100)
        print(f"  {span.text}")
        print_separator("Â·", 100)

    print()


def main():
    """ä¸»å‡½æ•°"""
    # åŠ è½½é…ç½®æ–‡ä»¶
    config_defaults = load_config()
    
    # è§£æå‘½ä»¤è¡Œå‚æ•°ï¼ˆå‘½ä»¤è¡Œå‚æ•°ä¼šè¦†ç›–é…ç½®æ–‡ä»¶ï¼‰
    args = parse_arguments(config_defaults)

    print_separator("=")
    print("ğŸ”¬ SparseAttribution (Chroma) ç›¸ä¼¼æ–‡æ¡£å½’å› æµ‹è¯•")
    print_separator("=")
    print(f"é…ç½®:")
    print(f"  æ»‘åŠ¨çª—å£å¤§å°: {args.window_size} tokens")
    print(f"  æ»‘åŠ¨çª—å£é‡å : {args.window_overlap} tokens")
    print(f"  æ‰“å° Top-N tokens: {args.top_n}")
    print(f"  Top-K ç‰‡æ®µ: {args.top_k_spans}")
    print(f"  ä½¿ç”¨ FP16: {not args.no_fp16}")
    print_separator("=")
    print()

    try:
        # ============== 1. åˆå§‹åŒ– ChromaStore ==============
        chroma_store = ChromaStore(persist_directory=args.persist_dir)

        collections = chroma_store.list_collections()
        if args.collection not in collections:
            print(f"âœ— é”™è¯¯: é›†åˆ '{args.collection}' ä¸å­˜åœ¨!")
            print(f"å¯ç”¨é›†åˆ: {', '.join(collections)}")
            return

        collection = chroma_store.get_collection(args.collection)
        total_docs = collection.count()
        if total_docs <= 0:
            print(f"âœ— é”™è¯¯: é›†åˆ '{args.collection}' ä¸ºç©ºï¼ˆcount=0ï¼‰")
            return

        # ============== 2. åˆ›å»ºé‡‡æ ·å™¨å¹¶æ‰§è¡Œé‡‡æ ·/æ£€ç´¢ ==============
        sampler = RandomQuerySampler(chroma_store, random_seed=args.random_seed)
        results = sampler.sample_and_query(
            collection_name=args.collection,
            n_results=args.n
        )

        ids = results["ids"][0]
        documents = results["documents"][0]
        metadatas = results["metadatas"][0]
        distances = results["distances"][0]

        if not ids or not documents:
            print("âœ— é”™è¯¯: æ£€ç´¢ç»“æœä¸ºç©º")
            return

        query_id = ids[0]
        query_doc = documents[0] or ""
        query_metadata = metadatas[0] or {}

        if not query_doc.strip():
            print(f"âœ— é”™è¯¯: Query æ–‡æ¡£å†…å®¹ä¸ºç©º (id={query_id})")
            return

        print_query_document(query_id, query_doc, query_metadata)

        # ============== 3. åˆå§‹åŒ– SparseAttribution ==============
        print("ğŸ“¥ æ­£åœ¨åŠ è½½ BGE-M3 æ¨¡å‹...")
        
        # æ„å»ºé…ç½®ï¼šä¼˜å…ˆä½¿ç”¨å‘½ä»¤è¡Œå‚æ•°ï¼Œå›é€€åˆ°é…ç½®æ–‡ä»¶
        config = {
            "model_name": args.model_name,
            "use_fp16": config_defaults.get("use_fp16", True) if not args.no_fp16 else False,
            "window_size": args.window_size,
            "window_overlap": args.window_overlap,
            "top_k_spans": args.top_k_spans,
        }
        
        # å¦‚æœé…ç½®æ–‡ä»¶æŒ‡å®šäº† deviceï¼Œä¹ŸåŠ å…¥é…ç½®
        if "device" in config_defaults:
            config["device"] = config_defaults["device"]
        
        attribution = SparseAttribution(config)
        print("âœ“ æ¨¡å‹åŠ è½½å®Œæˆ")
        print()

        # ============== 4. å¯¹æ¯ä¸ªç›¸ä¼¼æ–‡æ¡£æ‰§è¡Œ sparse attribution ==============
        print_separator("=")
        print(f"ğŸ”¬ åˆ†æ Top {args.n} ç›¸ä¼¼æ–‡æ¡£çš„ sparse attribution")
        print_separator("=")
        print()

        # i=0 æ˜¯ query æœ¬èº«ï¼Œä» i=1 å¼€å§‹æ‰æ˜¯ç›¸ä¼¼æ–‡æ¡£
        for i in range(1, len(ids)):
            doc_id = ids[i]
            document = documents[i] or ""
            metadata = metadatas[i] or {}
            distance = distances[i]

            if not document.strip():
                print(f"âš ï¸ è·³è¿‡ç©ºæ–‡æ¡£: id={doc_id}")
                continue

            # token-level æ‰“å°ç”¨ top tokens
            top_tokens = attribution.get_top_contributing_tokens(
                query_doc, document, top_n=args.top_n
            )

            # spansï¼šsliding window attribution
            attribution_result = attribution.extract(query_doc, document)

            print_similar_document_with_sparse_attribution(
                doc_index=i,
                doc_id=doc_id,
                document=document,
                metadata=metadata,
                distance=distance,
                attribution_result=attribution_result,
                top_tokens=top_tokens,
            )

        # ============== 5. æ€»ç»“ ==============
        print_separator("=")
        print("âœ… åˆ†æå®Œæˆ")
        print_separator("=")
        print(f"collection: {args.collection}")
        print(f"query_id: {query_id}")
        print(f"query_length: {len(query_doc)} chars")
        print(f"similar_docs_analyzed: {max(0, len(ids) - 1)}")
        if len(distances) > 1:
            print(f"distance_range: {distances[1]:.6f} ~ {distances[-1]:.6f}")
        print_separator("=")

    except Exception as e:
        print(f"\nâœ— å‘ç”Ÿé”™è¯¯: {e}")
        import traceback
        traceback.print_exc()
        sys.exit(1)


if __name__ == "__main__":
    main()
